# batchnorm如何计算

Batch Normalization（批归一化）的计算步骤如下：
1. **计算均值和方差**：对当前批次（batch）的每个特征通道分别计算均值和方差。
   $$
   \mu_B = \frac{1}{m} \sum_{i=1}^m x_i, \quad \sigma_B^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2
   $$
   （其中 $m$ 是批次样本数，$x_i$ 是某个通道的值）

2. **归一化**：对每个特征值进行归一化。
   $$
   \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
   $$
   （$\epsilon$ 是防止除零的小常数，如 `1e-5`）

3. **缩放和平移**：通过可学习的参数 $\gamma$ 和 $\beta$ 调整数据分布。
   $$
   y_i = \gamma \hat{x}_i + \beta
   $$



# layernorm 和 batchnorm的区别

Layer Normalization（层归一化）和 Batch Normalization（批归一化）是两种常用的归一化方法，核心区别在于**归一化的维度**和**适用场景**：

------

### 1. **归一化维度对比**

| **方法**      | **计算维度**                                                 | **示意图**                                                   |
| :------------ | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **BatchNorm** | 沿 **Batch维度** 归一化，对每个特征通道分别计算均值和方差。 （同一通道的所有样本归一化） | ![BatchNorm](https://miro.medium.com/v2/resize:fit:720/format:webp/1*6QlGPe6QvQ7S-3g9QO8baw.png) |
| **LayerNorm** | 沿 **特征维度** 归一化，对每个样本的所有特征计算均值和方差。 （同一样本的所有通道归一化） | ![LayerNorm](https://miro.medium.com/v2/resize:fit:720/format:webp/1*I0ws8qOFjQSFVWUJj10Zuw.png) |

**公式对比**：

- **BatchNorm**：

  μB=1B∑i=1Bxi,σ2B=1B∑i=1B(xi−μB)2μB=1B∑i=1Bxi,σB2=1B∑i=1B(xi−μB)2

  *μ**B*=*B*1*i*=1∑*B**x**i*,*σ**B*2=*B*1*i*=1∑*B*(*x**i*−*μ**B*)2

  （BB*B* 为批次大小，按特征通道独立计算）

- **LayerNorm**：

  μL=1D∑i=1Dxi,σ2L=1D∑i=1D(xi−μL)2μL=1D∑i=1Dxi,σL2=1D∑i=1D(xi−μL)2

  *μ**L*=*D*1*i*=1∑*D**x**i*,*σ**L*2=*D*1*i*=1∑*D*(*x**i*−*μ**L*)2

  （DD*D* 为特征维度，按样本独立计算）

------

### 2. **适用场景对比**

| **特性**          | **BatchNorm**                      | **LayerNorm**                        |
| :---------------- | :--------------------------------- | :----------------------------------- |
| **数据依赖**      | 依赖批次数据（Batch Size需足够大） | 单样本独立计算（对Batch Size不敏感） |
| **适用任务**      | 图像任务（CNN）                    | 序列任务（RNN/Transformer）          |
| **训练/测试差异** | 测试时需用移动平均的均值和方差     | 训练和测试计算方式一致               |
| **动态网络支持**  | 对动态结构（如变长序列）不友好     | 支持动态结构（如NLP中的变长输入）    |

------

### 3. **典型应用**

- **BatchNorm**：
  - 图像分类（如ResNet）
  - 目标检测（如Faster R-CNN）
  - 要求固定输入尺寸的任务
- **LayerNorm**：
  - 自然语言处理（如Transformer、BERT）
  - 语音识别（如WaveNet）
  - 小批量或变长序列任务

------

### 4. **优缺点总结**

| **方法**      | **优点**                         | **缺点**                          |
| :------------ | :------------------------------- | :-------------------------------- |
| **BatchNorm** | 减少内部协变量偏移，加速训练收敛 | 对Batch Size敏感，小批次效果差    |
| **LayerNorm** | 对Batch Size不敏感，支持动态输入 | 在图像任务中可能不如BatchNorm有效 |

------

### 5. **选择建议**

- **用BatchNorm**：当数据是固定尺寸（如图像）、Batch Size较大时。
- **用LayerNorm**：当数据是变长序列（如文本）、Batch Size较小或动态变化时。

# nms如何计算

非极大值抑制（NMS）用于目标检测中剔除冗余框，步骤如下：
1. **按置信度排序**：将所有候选框按分类/检测得分从高到低排序。
2. **选择最高分框**：选中当前最高分的框，加入最终结果列表。
3. **计算IoU并剔除**：计算该框与剩余框的IoU（交并比），剔除IoU超过设定阈值（如0.5）的框。
4. **重复**：对剩余框重复步骤2-3，直到所有框被处理。

**IoU公式**：
$$
IoU = \frac{A \cap B}{A \cup B}
$$
（$A$ 和 $B$ 是两个框的交集和并集面积）



# 手撕NMS过程





# 梯度下降是啥

梯度下降（Gradient Descent）是一种优化算法，通过**沿负梯度方向更新参数**来最小化损失函数：
1. **计算梯度**：求损失函数 $L(\theta)$ 对参数 $\theta$ 的梯度 $\nabla_\theta L(\theta)$。
2. **参数更新**：
   $$
   \theta \leftarrow \theta - \eta \cdot \nabla_\theta L(\theta)
   $$
   （$\eta$ 是学习率，控制步长）

**变体**：
- **批量梯度下降**（BGD）：使用全量数据计算梯度。
- **随机梯度下降**（SGD）：每次随机选一个样本更新。
- **小批量梯度下降**（Mini-batch SGD）：折中方案，常用批量大小（如32、64）。



# 交叉熵损失是啥

交叉熵损失（Cross-Entropy Loss）衡量预测概率分布与真实分布的差异，常用于分类任务。

**二分类交叉熵**：
$$
L = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]
$$
（$y_i \in \{0,1\}$ 是真实标签，$p_i$ 是预测概率）

**多分类交叉熵**：
$$
L = -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{i,c} \log(p_{i,c})
$$
（$C$ 是类别数，$y_{i,c}$ 是one-hot编码的真实标签，$p_{i,c}$ 是预测概率）

**特点**：对错误预测施加更大惩罚，与Softmax激活函数配合使用。



# 交叉熵的数学推导



# 交叉熵的代码手写



# sigmoid 的代码手写



# 手撕多头注意力



# ReLU为什么能缓解梯度消失


**ReLU（Rectified Linear Unit）** 的导数为：
- **正区间**：梯度为1，梯度恒定，避免因链式法则连乘导致的梯度衰减。
- **负区间**：梯度为0，神经元“死亡”（但稀疏性可能提升泛化能力）。

相比于 Sigmoid/Tanh 的饱和区梯度接近0，ReLU 在激活时保持较大梯度，缓解了深层网络中的梯度消失问题。


# Adam优化器原理


Adam（Adaptive Moment Estimation）结合了 **动量法（Momentum）** 和 **自适应学习率** 的思想：
1. **一阶矩估计（均值）**：类似动量，累积梯度方向。
2. **二阶矩估计（方差）**：自适应调整学习率，类似RMSProp。
3. **参数更新**：
   $$
   m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\
   v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
   \hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \\
   \theta_t = \theta_{t-1} - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
   $$
   （$\beta_1, \beta_2$ 为衰减率，默认0.9和0.999）


# AUC计算方法
AUC（Area Under ROC Curve）的计算方法：
1. **梯形法则**：按不同阈值计算TPR和FPR，绘制ROC曲线后积分。
2. **排序法**（更高效）：
   - 将所有样本按预测概率从高到低排序。
   - 统计正样本排在负样本前的概率：
     $$
     \text{AUC} = \frac{\sum \text{正样本排名} - \frac{M(M+1)}{2}}{M \times N}
     $$
     （$M$ 为正样本数，$N$ 为负样本数）


# python装饰器作用
装饰器（Decorator）用于**动态扩展函数/类的功能**，而无需修改其源代码：
- **应用场景**：日志记录、权限校验、性能测试、事务处理等。
- **本质**：高阶函数，接受函数作为参数并返回新函数。
  

**示例**：
```python
def log_decorator(func):
    def wrapper(*args, **kwargs):
        print(f"Calling {func.__name__}")
        return func(*args, **kwargs)
    return wrapper

@log_decorator
def my_function():
    pass
```

# KL散度

KL散度（Kullback-Leibler Divergence）衡量两个概率分布 *P* 和 *Q* 的差异：

![image-20250423153245332](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423153245332.png)

# softmax 公式

![image-20250423153253527](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423153253527.png)

# 梯度消失和梯度爆炸如何缓解

1. **梯度消失**：
   - 使用ReLU等非饱和激活函数。
   - 残差连接（如ResNet）。
   - 合适的权重初始化（如He初始化）。
   - Batch Normalization。
2. **梯度爆炸**：
   - 梯度裁剪（Gradient Clipping）。
   - 权重正则化（L2正则）。
   - 使用更小的学习率。

# L1和L2正则的区别

| **特性**         | **L1正则（Lasso）**                                          | **L2正则（Ridge）**                                          |
| :--------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **惩罚项**       | ![image-20250423153320146](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423153320146.png) | ![image-20250423153327657](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423153327657.png) |
| **解的特性**     | 稀疏解（部分权重为0）                                        | 非稀疏解，权重接近0                                          |
| **抗过拟合能力** | 适合特征选择                                                 | 适合防止过拟合                                               |
| **计算特性**     | 非光滑，优化需特殊处理（如坐标下降）                         | 光滑，易梯度下降优化                                         |

# BN中可学习参数如何获取

BatchNorm 的可学习参数 **\*γ\*（缩放）** 和 **\*β\*（平移）**：

- **来源**：通过反向传播自动学习，初始值通常为 γ*=1, *β*=0。
- **作用**：恢复数据表达能力（防止归一化后信息损失）。

# 如何缓解过拟合

1. **数据层面**：
   - 增加训练数据（或数据增强）。
   - 降维或特征选择。
2. **模型层面**：
   - 简化模型结构（减少参数量）。
   - 正则化（L1/L2/Dropout）。
3. **训练策略**：
   - 早停（Early Stopping）。
   - 交叉验证。
   - 集成学习（Bagging/Boosting）。

# 介绍一下dropout

**Dropout** 是一种正则化技术，核心思想是**训练时随机丢弃神经元**：

1. **训练阶段**：
   - 每个神经元以概率 *p* 被临时丢弃（输出置0）。
   - 剩余神经元的输出需缩放：![image-20250423153431715](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423153431715.png)。
2. **测试阶段**：
   - 所有神经元激活，权重按 ![image-20250423153440573](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423153440573.png)。

**作用**：

- 减少神经元间的共适应性，增强泛化能力。
- 相当于隐式的模型平均。

# transformer基本原理

Transformer 是一种基于**自注意力机制**的神经网络架构，主要用于序列建模（如机器翻译、文本生成等），核心设计如下：

------

### 1. **整体结构**

Transformer 由 **编码器（Encoder）** 和 **解码器（Decoder）** 堆叠组成：

- **编码器**：将输入序列映射为隐含表示。
- **解码器**：根据编码器输出和已生成结果预测下一个元素。



------

### 2. **核心组件**

#### (1) 自注意力机制（Self-Attention）

通过计算序列中每个位置与其他位置的关联权重，捕捉全局依赖关系。

**计算步骤**：

1. **生成Q/K/V矩阵**：输入序列通过线性变换生成查询（Query）、键（Key）、值（Value）矩阵。

2. **计算注意力分数**：

   ![image-20250423152518939](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423152518939.png)

#### (2) 多头注意力（Multi-Head Attention）

并行计算多个自注意力头，增强模型对不同子空间特征的捕捉能力：

![image-20250423152624408](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423152624408.png)

#### (3) 位置编码（Positional Encoding）

为序列注入位置信息（因Transformer无时序结构），常用正弦/余弦函数：

![image-20250423152635410](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423152635410.png)

#### (4) 前馈网络（Feed-Forward Network）

每个位置的独立全连接层：

![image-20250423152644118](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423152644118.png)

#### (5) 残差连接与层归一化

- **残差连接**：缓解梯度消失，如 ![image-20250423152655677](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250423152655677.png)。
- **层归一化（LayerNorm）**：加速训练，稳定梯度。

------

### 3. **编码器与解码器差异**

| **组件**     | **编码器**     | **解码器**                             |
| :----------- | :------------- | :------------------------------------- |
| 自注意力层   | 普通多头注意力 | 带掩码的多头注意力（防止未来信息泄露） |
| 交叉注意力层 | 无             | 有（关注编码器输出）                   |

------

### 4. **特点**

- **并行计算**：无RNN的时序依赖，可并行处理序列。
- **长程依赖**：自注意力直接建模任意距离的依赖关系。
- **可扩展性**：通过堆叠层数提升模型容量。

------

### 5. **典型应用**

- BERT（仅编码器）
- GPT系列（仅解码器）
- 机器翻译（编码器-解码器联合）

# 注意力机制计算方法和原理