## batchnorm如何计算



## nms如何计算



## 梯度下降是啥



## 交叉熵损失是啥



## transformer基本原理



## 注意力机制计算方法和原理

# 分类任务loss

本项目仅使用交叉熵损失（Cross-Entropy loss），仅需要对类别预测进行loss。

## softmax

![img](https://i-blog.csdnimg.cn/blog_migrate/574b31a8090423bf792117896a9d962f.png)

Z 表示输出层神经元的值，指数充当非线性函数。随后将这些值除以指数值之和以进行标准化，然后将其转换为概率。

**Softmax的含义就在于不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。**

## 交叉熵

交叉熵是给定随机变量或事件集的两个概率分布之间差异的度量。

交叉熵刻画了两个概率分布之间的距离，旨在描绘通过概率分布 q来表达概率分布 p的困难程度。根据公式不难理解，**交叉熵越小，两个概率分布 p和 q越接近。**

以三类分类问题为例，假设数据 x属于类别 1。记数据x的类别分布概率为 y，显然 y=(1,0,0)代表数据 x的实际类别分布概率。记![\hat{y}](https://latex.csdn.net/eq?%5Chat%7By%7D)代表模型预测所得类别分布概率。那么对于数据 x而言，其实际类别分布概率 y和模型预测类别分布概率![\hat{y}](https://latex.csdn.net/eq?%5Chat%7By%7D)的交叉熵损失函数定义为：

![crossEntropy =-ylog{\hat{y}}](https://latex.csdn.net/eq?crossEntropy%20%3D-ylog%7B%5Chat%7By%7D%7D)

神经网络所预测类别分布概率与实际类别分布概率之间的差距越小越好，即**交叉熵越小越好**。

# 激活函数

## ReLU（Rectified Linear Unit）

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/a2627a5ddb4cd9821fdbc8ffbf828d2a.png)

ReLU函数在**输入大于0时直接输出，否则输出0**。它的数学形式为f(x) = max(0, x)，可以看作是一个分段函数，具有非常好的计算性质，使得神经网络的训练更加高效。

## Leaky ReLU

Leaky ReLU是ReLU的一种变体，改变之处在于 **负数的输出不再是0了，而是一个很小的数值，比如0.1或0.01**。

**优点**在于：可以避免出现“神经元死亡”的情况，即在训**练过程中某些神经元的输出始终为0**，从而导致无法更新其权重

## FReLU（Flatten ReLU）

FReLU将输入展平（flatten）成一个一维向量，然后对每个元素应用ReLU激活函数，最后再将输出重新恢复成原来的形状。

**优点：**

1. 减少参数量：FReLU不需要额外的参数，因此可以减少模型的参数量。
2. 具有更好的表示能力：由于FReLU可以将输入展平成一维向量，因此可以在不增加参数量的情况下提高模型的表示能力。
3. 提高模型的鲁棒性：由于FReLU对输入进行了展平操作，因此可以提高模型对输入的鲁棒性，从而减少过拟合的风险。

## [SiLU](https://so.csdn.net/so/search?q=SiLU&spm=1001.2101.3001.7020)（Sigmoid Linear Unit）

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/4bc88d2348d583953e180e9278fefba8.png)

相对于ReLU函数，**SiLU函数在接近零时具有更平滑的曲线**，并且由于其使用了sigmoid函数，可以**使网络的输出范围在0和1之间**。这使得SiLU在一些应用中比ReLU表现更好，例如在语音识别中使用SiLU比ReLU可以取得更好的效果。

### 注意

在使用SiLU时，如果**数据存在过大或过小**的情况，可能会导致**梯度消失或梯度爆炸**，因此需要进行一些调整，例如对输入数据进行归一化等。而ReLU在这方面较为稳定，不需要过多的处理。

相较于ReLU函数，SiLU函数可能会更适合一些需要保留更多输入信息的场景。

# 评价指标

## mAP50(平均精度均值@IoU=0.5)

在交并比（IoU）阈值为0.5时，所有类别的平均精度（AP）均值。

- **IoU计算**：预测框与真实框的交集面积与并集面积之比。

- 阈值标准：仅当预测框与真实框的IoU≥0.5时，视为检测正确。

  ![image-20250415124822934](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250415124822934.png)

  

## precision

预测为正例的样本中，实际为正例的比例，反映模型预测的准确性。

![image-20250415124607088](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250415124607088.png)

- **TP**：真正例（预测正确且实际为正例）
- **FP**：假正例（预测错误，实际为负例但被误判为正例）

## recall

实际为正例的样本中，被正确预测为正例的比例，反映模型的查全能力。

![image-20250415124647885](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250415124647885.png)

- **FN**：假反例（预测错误，实际为正例但被漏检）

## mAP50-95(平均精度均值@IoU=0.5:0.95)

在IoU阈值从0.5到0.95（步长0.05）范围内，所有类别的平均精度均值。

**计算步骤**：

1. 对每个类别，在10个IoU阈值（0.5, 0.55, ..., 0.95）下分别计算AP。
2. 对所有类别的AP取平均，再对10个阈值下的结果取均值

![image-20250415124939915](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250415124939915.png)

## AP

平均精度（Average Precision, AP）通过综合评估不同置信度阈值下的精确率（Precision）和召回率（Recall）来计算。

AP是**精确率-召回率（PR）曲线下的面积**

- AP值越高，模型在兼顾精确率和召回率上的表现越好。

# 90-100fps

![image-20250415133048518](C:\Users\gaojixue\AppData\Roaming\Typora\typora-user-images\image-20250415133048518.png)
